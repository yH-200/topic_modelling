{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1918e79b",
   "metadata": {},
   "source": [
    "### Cleaning the HateXplain Dataset\n",
    "The datasets are already cleaned and stored under Task1_Outputs. <br>\n",
    "The folder contains: \n",
    "1) Binary Dataset folder for instances trained on the binary dataset <br>\n",
    "2) Ternary Dataset folder for instances trained on the ternary dataset\n",
    "\n",
    "Each folder contains two folders for instances with topic covariates and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ec958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HateXplain dataset using Huggingface\n",
    "hf_dataset = load_dataset('hatexplain', 'plain_text')\n",
    "\n",
    "# Load the train split\n",
    "hf_train_dataset = hf_dataset[\"train\"]\n",
    "\n",
    "# Load the test and validation split\n",
    "hf_test_dataset = concatenate_datasets([hf_dataset[\"test\"], hf_dataset[\"validation\"]])\n",
    "\n",
    "# Display the features\n",
    "hf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hf_train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c35ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[27, \"rationales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if df.loc[i, \"annotators\"][\"label\"][0] == 1 and df.loc[i, \"annotators\"][\"label\"][1] == 1 and df.loc[i, \"annotators\"][\"label\"][2] == 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[45, \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "why should you ban immigrants from operating small business every business start somewhere and tomorrow that will give people job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the label assigned by most annotators\n",
    "def most_common_label(numpy_list):\n",
    "    return np.argmax(np.bincount(numpy_list))\n",
    "\n",
    "# Convert the ternary list of labels (hateful, normal, offensive) to binary (normal, toxic)\n",
    "def convert_ternary_list_to_binary(labels_ternary_list):\n",
    "    labels_binary_list = []\n",
    "    for label in labels_ternary_list:\n",
    "        if label == 0 or label == 2:\n",
    "            label = 1 # toxic\n",
    "        else:\n",
    "            label = 0 # normal\n",
    "        labels_binary_list.append(label)\n",
    "    return labels_binary_list\n",
    "\n",
    "# Extract the aspects from the dataset that were selected by at least 2 annotators \n",
    "def identify_aspects(aspects_list, text):\n",
    "    aspects = []\n",
    "#     Only 2 annotators marked the text as toxic\n",
    "    if len(aspects_list) == 2:\n",
    "        array1 = aspects_list[0]\n",
    "        array2 = aspects_list[1]\n",
    "        for i in range(len(text)):\n",
    "#             Find the tokens that were selected by both\n",
    "            if array1[i] and array2[i]:\n",
    "                aspects.append(text[i])\n",
    "#     3 annotators marked the text as toxic\n",
    "    if len(aspects_list) == 3:\n",
    "        array1 = aspects_list[0]\n",
    "        array2 = aspects_list[1]\n",
    "        array3 = aspects_list[2]\n",
    "        for i in range(len(text)):\n",
    "#             Find the tokens that were selected by at least 2 annotators\n",
    "            if array1[i] and array2[i] or array1[i] and array3[i] or array2[i] and array3[i]:\n",
    "                aspects.append(text[i])\n",
    "    return aspects\n",
    "\n",
    "# Define the maximum number of extracted aspects for each document and add columns in the dataset for each aspect\n",
    "def add_aspects_columns(pandas_dataset):\n",
    "#     Max number of aspects\n",
    "    num_col = 4\n",
    "    aspect_list = pandas_dataset[\"aspects\"]\n",
    "    for i in range(num_col):\n",
    "        col_list = []\n",
    "        for array in aspect_list:\n",
    "            try:\n",
    "                col_list.append(array[i])\n",
    "            except IndexError:\n",
    "                col_list.append(\"\")\n",
    "#         Add a new column to the dataset with the aspects\n",
    "        pandas_dataset[\"aspect_\"+str(i+1)] = col_list\n",
    "\n",
    "# Prepare the binary dataset (convert ternary dataset to binary, extract aspects, remove emojis & user tokens)  \n",
    "def prepare_binary_dataset(pandas_dataset):\n",
    "    for i in range(len(pandas_dataset)):\n",
    "#         Set IDs\n",
    "        pandas_dataset.loc[i, \"id\"] = i\n",
    "#         Set binary label\n",
    "        pandas_dataset.loc[i, \"label\"] = most_common_label(convert_ternary_list_to_binary(pandas_dataset.loc[i, \"label\"][\"label\"]))\n",
    "#         Identify the aspects of each tweet\n",
    "        pandas_dataset.loc[i, \"aspects\"] = identify_aspects(pandas_dataset.loc[i, \"aspects\"], pandas_dataset.loc[i, \"text\"])\n",
    "#         Merge tokens into a string and remove emojis and <user>\n",
    "        word_list = [word for word in pandas_dataset.loc[i, \"text\"] if word.isalpha()]\n",
    "        text = \" \".join(word_list)\n",
    "        pandas_dataset.loc[i, \"text\"] = text\n",
    "#     Add new columns to the dataset with the aspects\n",
    "    add_aspects_columns(pandas_dataset)    \n",
    "\n",
    "# Prepare the ternary dataset\n",
    "def prepare_ternary_dataset(pandas_dataset):\n",
    "    for i in range(len(pandas_dataset)):\n",
    "#         Set IDs\n",
    "        pandas_dataset.loc[i, \"id\"] = i\n",
    "#         Set ternary label\n",
    "        pandas_dataset.loc[i, \"label\"] = most_common_label(pandas_dataset.loc[i, \"label\"][\"label\"])\n",
    "#         Identify the aspects of each tweet\n",
    "        pandas_dataset.loc[i, \"aspects\"] = identify_aspects(pandas_dataset.loc[i, \"aspects\"], pandas_dataset.loc[i, \"text\"])\n",
    "#         Merge tokens into a string and remove emojis and <user>\n",
    "        word_list = [word for word in pandas_dataset.loc[i, \"text\"] if word.isalpha()]\n",
    "        text = \" \".join(word_list)\n",
    "        pandas_dataset.loc[i, \"text\"] = text\n",
    "#     Add new columns to the dataset with the aspects\n",
    "    add_aspects_columns(pandas_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset column headings (train)\n",
    "new_train_dataset = hf_train_dataset.rename_column(\"annotators\", \"label\")\n",
    "new_train_dataset.rename_column_(\"post_tokens\", \"text\")\n",
    "new_train_dataset.rename_column_(\"rationales\", \"aspects\")\n",
    "\n",
    "# Change the dataset column headings (test)\n",
    "new_test_dataset = hf_test_dataset.rename_column(\"annotators\", \"label\")\n",
    "new_test_dataset.rename_column_(\"post_tokens\", \"text\")\n",
    "new_test_dataset.rename_column_(\"rationales\", \"aspects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3148901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataset\n",
    "binary_train_dataset = new_train_dataset.to_pandas()\n",
    "binary_test_dataset = new_test_dataset.to_pandas()\n",
    "\n",
    "# Prepare the binary dataset\n",
    "prepare_binary_dataset(binary_train_dataset)\n",
    "prepare_binary_dataset(binary_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataset\n",
    "ternary_train_dataset = new_train_dataset.to_pandas()\n",
    "ternary_test_dataset = new_test_dataset.to_pandas()\n",
    "\n",
    "# Prepare the binary dataset\n",
    "prepare_ternary_dataset(ternary_train_dataset)\n",
    "prepare_ternary_dataset(ternary_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839407f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_dataset.to_json(\"Task1_Outputs/Binary_Dataset/train.jsonlist\", orient=\"records\", lines=True)\n",
    "binary_test_dataset.to_json(\"Task1_Outputs/Binary_Dataset/test.jsonlist\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_train_dataset.to_json(\"Task1_Outputs/Ternary_Dataset/train.jsonlist\", orient=\"records\", lines=True)\n",
    "ternary_test_dataset.to_json(\"Task1_Outputs/Ternary_Dataset/test.jsonlist\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
